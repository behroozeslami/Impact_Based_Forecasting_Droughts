{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-825b2d51a0e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapclient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import ee.mapclient\n",
    "from geetools import batch\n",
    "from geetools import tools\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import quandl\n",
    "\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "\"\"\"\n",
    "Author: Joris Westerveld\n",
    "file containing the necessary functions needed to extract data from the Google Earth Engine\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that can extract and spatial reduce \n",
    "the data from the Google Earth Engine.\n",
    "\n",
    "It returns a feature collection containing \n",
    "the data that is reduced per region.\n",
    "\"\"\"\n",
    "\n",
    "def extract_data_EE(im_col, fe_col, min_year, max_year, min_month, max_month, reducer_func, scale = 1000, export = False, reduce_imcol = True):\n",
    "    # initialize earth engine\n",
    "    ee.Initialize()\n",
    "\n",
    "    # create list \n",
    "    year_data = []\n",
    "    \n",
    "    # since the range works betwen values this compensates so that when you want the max year to be 2018, it will be untill 2019 (thus 2018)\n",
    "    max_year = max_year + 1\n",
    "    max_month = max_month + 1\n",
    "    \n",
    "    # from 2010-2018:\n",
    "    for yNum in range(min_year, max_year):\n",
    "        month_data = []\n",
    "\n",
    "        # from january - december\n",
    "        for mNum in range(min_month, max_month):\n",
    "            \n",
    "            # load data\n",
    "            \n",
    "            if reduce_imcol == True:\n",
    "                \n",
    "                # reduce the image collection to one image:\n",
    "                imageCol = ee.ImageCollection(im_col).filter(ee.Filter.calendarRange(yNum,yNum,'year')).filter(ee.Filter.calendarRange(mNum,mNum,'month'))\n",
    "                reduceImageCol = imageCol.reduce(ee.Reducer.sum()) #Sum for precipitation to sum the month\n",
    "\n",
    "            else:\n",
    "                # already a single image, no need to reduce:\n",
    "                reduceImageCol = ee.Image(im_col)\n",
    "           \n",
    "            # load regions: \n",
    "            woreda = ee.FeatureCollection(fe_col)\n",
    "            \n",
    "            # get mean values by woreda polygon\n",
    "            imageCol_spatial_reduction = reduceImageCol.reduceRegions(collection = woreda, \n",
    "                                                   reducer = reducer_func, \n",
    "                                                   scale = scale)\n",
    "\n",
    "            def newCol(feature):\n",
    "                feature = feature.set('Year',yNum)\n",
    "                feature = feature.set('Month',mNum)\n",
    "                return(feature)\n",
    "\n",
    "            # add a new column for year to each feature in the feature collection\n",
    "            polyOut = imageCol_spatial_reduction.map(newCol)\n",
    "\n",
    "            month_data.append(polyOut)\n",
    "\n",
    "        year_data.append(month_data)\n",
    "\n",
    "    if export == True:\n",
    "            \n",
    "        # If you want to EXPORT csv to your google drive:\n",
    "        # Table to Drive Export Example\n",
    "        mytask  = ee.batch.Export.table.toDrive(collection = polyOut, \n",
    "                                                description = 'out',\n",
    "                                                folder = im_col,\n",
    "                                                fileFormat = 'CSV')  \n",
    "        ee.batch.data.startProcessing(mytask.id, mytask.config)\n",
    "            \n",
    "    return(year_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that can convert a feature collection \n",
    "created through the google earth engine to a pandas DataFrame\n",
    "\"\"\"\n",
    "\n",
    "# function to convert a feature collection to a data frame \n",
    "def fc_to_df(year_data):\n",
    "    data_list = []\n",
    "    \n",
    "    # for every (month) feature collection in the year feature collection:\n",
    "    for data in year_data:\n",
    "        \n",
    "        # since 2018-12 is an empty feature collection at this moment this doesn't exist as so catch this error\n",
    "        # this might nog be the prettiest solution.\n",
    "        try:\n",
    "            features = data.getInfo()['features']\n",
    "            dict_list = []\n",
    "        except:\n",
    "            print(\"No info, month skipped\")\n",
    "            continue\n",
    "            # return data_list\n",
    "            \n",
    "        \n",
    "        # if it contains features, than for every feature add it to a list\n",
    "        for f in features:\n",
    "            attribute = f['properties']\n",
    "            dict_list.append(attribute)\n",
    "        \n",
    "        # when each feature has been done add to list for one month\n",
    "        df = pd.DataFrame(dict_list)\n",
    "        data_list.append(df)\n",
    "    \n",
    "    # returning a list with a df for each month of the feature collection\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that turns a dictionary containing feature collections\n",
    "into a dictionary containing data frames.\n",
    "\"\"\"\n",
    "\n",
    "def fcdict_to_df(start_year, fe_col):\n",
    "\n",
    "    # script to add each month in the feature collection to a dictonairy:\n",
    "    df_dict = {}\n",
    "    \n",
    "    # this takes some time. For 8 years * 12 months it takes around 30 - 60 minutes. \n",
    "    \n",
    "    # for every datapoint in fe_col (feature collection for all years, all months):\n",
    "    for data in fe_col:\n",
    "\n",
    "        # dummy variable in order to keep account where the script is and for the key of the dictonairy\n",
    "        print(start_year)\n",
    "\n",
    "        # convert to dataframe:\n",
    "        data = fc_to_df(data)\n",
    "        \n",
    "        try:\n",
    "            # Concat the dataframes:\n",
    "            df_dict['{0}'.format(start_year)] = pd.concat(data)\n",
    "            print('concat')\n",
    "            \n",
    "        except ValueError:\n",
    "            print('no data to append in {0}, skipping this year'.format(start_year))\n",
    "            pass\n",
    "\n",
    "        start_year = start_year + 1\n",
    "        \n",
    "    # Turn dataframe dict to one dataframe:\n",
    "    df_result = pd.concat(df_dict.values(), ignore_index=True)\n",
    "    \n",
    "    return(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for the fusion tables:\n",
    "#country = ee.FeatureCollection('ft:12FZSu0lre4EtJdK7KvUYoTKw-A7B5A-f1iiAPHvz') #Uganda ADMI_0 shapefile\n",
    "country = ee.FeatureCollection('ft:1D-opj7oMapOL0H__HwezRFCK0lzQW9aqPpmNnp4C') #Uganda ADM_1\n",
    "#country = ee.FeatureCollection('ft:1HuJw6zpefZhhNRWc_fcqvNnn5HOoqBEZ4IXy2bdl') #Kenya ADM_1\n",
    "#country = ee.FeatureCollection('ft:1HftUoTqcEQyJwDRIhx7KLyb4_ss81QZCjuh9vwRa') #Ethiopia ADM_2\n",
    "#country = ee.FeatureCollection('ft:1IdlUSTXrxgwIR2SCuptBJdGuTL6jTlw3cVvjBo-f') #Ethiopia ADM_1\n",
    " \n",
    "\n",
    "# country abbreviation:\n",
    "country_abb = \"UG\"  #UG #KE #UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "                                 NDVI mean                                               \n",
      "-----------------------------------------------------------------------------------------\n",
      "1981\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "concat\n",
      "1982\n",
      "concat\n",
      "1983\n",
      "concat\n",
      "1984\n",
      "concat\n",
      "1985\n",
      "concat\n",
      "1986\n",
      "concat\n",
      "1987\n",
      "concat\n",
      "1988\n",
      "concat\n",
      "1989\n",
      "concat\n",
      "1990\n",
      "concat\n",
      "1991\n",
      "concat\n",
      "1992\n",
      "concat\n",
      "1993\n",
      "concat\n",
      "1994\n",
      "concat\n",
      "1995\n",
      "concat\n",
      "1996\n",
      "No info, month skipped\n",
      "concat\n",
      "1997\n",
      "concat\n",
      "1998\n",
      "concat\n",
      "1999\n",
      "concat\n",
      "2000\n",
      "concat\n",
      "2001\n",
      "concat\n",
      "2002\n",
      "concat\n",
      "2003\n",
      "concat\n",
      "2004\n",
      "concat\n",
      "2005\n",
      "concat\n",
      "2006\n",
      "concat\n",
      "2007\n",
      "concat\n",
      "2008\n",
      "concat\n",
      "2009\n",
      "concat\n",
      "2010\n",
      "concat\n",
      "2011\n",
      "concat\n",
      "2012\n",
      "concat\n",
      "2013\n",
      "concat\n",
      "2014\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "no data to append in 2014, skipping this year\n",
      "2015\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "no data to append in 2015, skipping this year\n",
      "2016\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "no data to append in 2016, skipping this year\n",
      "2017\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "no data to append in 2017, skipping this year\n",
      "2018\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "no data to append in 2018, skipping this year\n",
      "2019\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "No info, month skipped\n",
      "no data to append in 2019, skipping this year\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"                                 NDVI mean                                               \")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Collect Features:\n",
    "fc_ndvi_mean = extract_data_EE(im_col  = \"NASA/GIMMS/3GV0\", \n",
    "                     fe_col  = country, min_year = 1981, max_year = 2019, min_month = 1, \n",
    "                     max_month = 12, reducer_func = ee.Reducer.mean())\n",
    "\n",
    "# Turn feature collection dict to a single dataframe\n",
    "df_ndvi_mean = fcdict_to_df(1981, fc_ndvi_mean)\n",
    "df_ndvi_mean = df_ndvi_mean.rename(columns={'mean':'ndvi_mean'})\n",
    "\n",
    "df_ndvi_mean.to_csv(r\"C:\\Users\\MPanis\\Documents\\510 Documents\\UG_ndvi_adm_1.csv\", sep= ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "                                 CHIRPS mean                                             \n",
      "-----------------------------------------------------------------------------------------\n",
      "1981\n",
      "concat\n",
      "1982\n",
      "concat\n",
      "1983\n",
      "concat\n",
      "1984\n",
      "concat\n",
      "1985\n",
      "concat\n",
      "1986\n",
      "concat\n",
      "1987\n",
      "concat\n",
      "1988\n",
      "concat\n",
      "1989\n",
      "concat\n",
      "1990\n",
      "concat\n",
      "1991\n",
      "concat\n",
      "1992\n",
      "concat\n",
      "1993\n",
      "concat\n",
      "1994\n",
      "concat\n",
      "1995\n",
      "concat\n",
      "1996\n",
      "concat\n",
      "1997\n",
      "concat\n",
      "1998\n",
      "concat\n",
      "1999\n",
      "concat\n",
      "2000\n",
      "concat\n",
      "2001\n",
      "concat\n",
      "2002\n",
      "concat\n",
      "2003\n",
      "concat\n",
      "2004\n",
      "concat\n",
      "2005\n",
      "concat\n",
      "2006\n",
      "concat\n",
      "2007\n",
      "concat\n",
      "2008\n",
      "concat\n",
      "2009\n",
      "concat\n",
      "2010\n",
      "concat\n",
      "2011\n",
      "concat\n",
      "2012\n",
      "concat\n",
      "2013\n",
      "concat\n",
      "2014\n",
      "concat\n",
      "2015\n",
      "concat\n",
      "2016\n",
      "concat\n",
      "2017\n",
      "concat\n",
      "2018\n",
      "concat\n"
     ]
    }
   ],
   "source": [
    "#Note: Change to sum above (reduce_imcol)!!!!\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"                                 CHIRPS mean                                             \")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Collect Features:\n",
    "fc_p_sum = extract_data_EE(im_col  = \"UCSB-CHG/CHIRPS/DAILY\", \n",
    "                     fe_col  = country, min_year = 1981, max_year = 2018, min_month = 1, \n",
    "                     max_month = 12, reducer_func = ee.Reducer.mean())\n",
    "\n",
    "# Turn feature collection dict to a single dataframe\n",
    "df_p_sum = fcdict_to_df(1981, fc_p_sum)\n",
    "df_p_sum = df_p_sum.rename(columns={'sum':'precipitation'})\n",
    "\n",
    "df_p_sum.to_csv(r\"C:\\Users\\MPanis\\Documents\\510 Documents\\UG_p_sum_adm_1.csv\", sep= ';', index = False)\n",
    "\n",
    "#Average per day the amount of rainfall in each district, than sum the average daily value over the month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "                                 Land Surface Temperature mean                           \n",
      "-----------------------------------------------------------------------------------------\n",
      "2000\n",
      "No info, month skipped\n",
      "concat\n",
      "2001\n",
      "concat\n",
      "2002\n",
      "concat\n",
      "2003\n",
      "concat\n",
      "2004\n",
      "concat\n",
      "2005\n",
      "concat\n",
      "2006\n",
      "concat\n",
      "2007\n",
      "concat\n",
      "2008\n",
      "concat\n",
      "2009\n",
      "concat\n",
      "2010\n",
      "concat\n",
      "2011\n",
      "concat\n",
      "2012\n",
      "concat\n",
      "2013\n",
      "concat\n",
      "2014\n",
      "concat\n",
      "2015\n",
      "concat\n",
      "2016\n",
      "concat\n",
      "2017\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"                                 Land Surface Temperature mean                           \")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Collect Features:\n",
    "fc_lst_mean = extract_data_EE(im_col  = \"MODIS/006/MOD11A1\", \n",
    "                     fe_col  = country, min_year = 2000, max_year = 2018, min_month = 1, \n",
    "                     max_month = 12, reducer_func = ee.Reducer.mean())\n",
    "\n",
    "# Turn feature collection dict to a single dataframe\n",
    "df_lst_mean = fcdict_to_df(2000, fc_lst_mean)\n",
    "\n",
    "\n",
    "df_lst_mean.to_csv(r\"C:\\Users\\MPanis\\Documents\\510 Documents\\UG_lst_adm_1.csv\", sep= ';', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "No info, month skipped\n",
      "concat\n",
      "2001\n",
      "concat\n",
      "2002\n",
      "No info, month skipped\n",
      "concat\n",
      "2003\n",
      "concat\n",
      "2004\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "                                 soil moisture mean                                      \n",
      "-----------------------------------------------------------------------------------------\n",
      "2010\n",
      "concat\n",
      "2011\n",
      "concat\n",
      "2012\n",
      "concat\n",
      "2013\n",
      "concat\n",
      "2014\n",
      "concat\n",
      "2015\n",
      "concat\n",
      "2016\n",
      "concat\n",
      "2017\n",
      "concat\n",
      "2018\n",
      "concat\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "print(\"                                 soil moisture mean                                      \")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Collect Features:\n",
    "fc_soilmois_mean = extract_data_EE(im_col  = \"NASA_USDA/HSL/soil_moisture\", \n",
    "                   fe_col  = country, min_year = 2010,max_year = 2018,min_month = 1,\n",
    "                   max_month = 12,reducer_func = ee.Reducer.mean())\n",
    "\n",
    "# Turn feature collection dict to a single dataframe\n",
    "df_soilmois_mean = fcdict_to_df(2010, fc_soilmois_mean)\n",
    "\n",
    "# save NDVI dataframe to .CSV\n",
    "df_soilmois_mean.to_csv(r\"C:\\Users\\MPanis\\Documents\\510 Documents\\UG_sm_adm_1.csv\", sep= ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndvi_mean.to_csv(r\"C:\\Users\\MPanis\\Documents\\510 Documents\\ET_ndvi_adm_2.csv\", sep= ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['Month', 'Year', 'ndvi_mean']\n",
    "df_ndvi_mean[cols_to_keep].to_csv(r\"C:\\Users\\MPanis\\Documents\\510 Documents\\UG_ndvi.csv\", sep= ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADM0_EN</th>\n",
       "      <th>ADM0_EN_ft_style</th>\n",
       "      <th>ADM0_PCODE</th>\n",
       "      <th>ADM0_PCODE_ft_style</th>\n",
       "      <th>ADM1_EN</th>\n",
       "      <th>ADM1_PCODE</th>\n",
       "      <th>ADM2ALT1EN</th>\n",
       "      <th>ADM2ALT2EN</th>\n",
       "      <th>ADM2_EN</th>\n",
       "      <th>ADM2_PCODE</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>date_ft_style</th>\n",
       "      <th>geometry_pos</th>\n",
       "      <th>geometry_vertex_count</th>\n",
       "      <th>import_notes</th>\n",
       "      <th>ndvi_mean</th>\n",
       "      <th>qa_mean</th>\n",
       "      <th>validOn</th>\n",
       "      <th>validOn_ft_style</th>\n",
       "      <th>validTo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15283</th>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ET</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Afar</td>\n",
       "      <td>ET02</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Zone 1 (Awsi Rasu)</td>\n",
       "      <td>ET0201</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.199624, 1...</td>\n",
       "      <td>1769.0</td>\n",
       "      <td></td>\n",
       "      <td>0.185094</td>\n",
       "      <td>1.037139</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15284</th>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ET</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Afar</td>\n",
       "      <td>ET02</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Zone 2 (Kilbet Rasu)</td>\n",
       "      <td>ET0202</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.5829508, ...</td>\n",
       "      <td>986.0</td>\n",
       "      <td></td>\n",
       "      <td>0.192246</td>\n",
       "      <td>1.111855</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15285</th>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ET</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Afar</td>\n",
       "      <td>ET02</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Zone 3 (Gabi Rasu)</td>\n",
       "      <td>ET0203</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.6370475, ...</td>\n",
       "      <td>2366.0</td>\n",
       "      <td></td>\n",
       "      <td>0.317286</td>\n",
       "      <td>1.010033</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15286</th>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ET</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Afar</td>\n",
       "      <td>ET02</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Zone 4 (Fantana Rasu)</td>\n",
       "      <td>ET0204</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.2679372, ...</td>\n",
       "      <td>1366.0</td>\n",
       "      <td></td>\n",
       "      <td>0.204236</td>\n",
       "      <td>1.001729</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15287</th>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ET</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Afar</td>\n",
       "      <td>ET02</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Zone 5 (Hari Rasu)</td>\n",
       "      <td>ET0205</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.3595423, ...</td>\n",
       "      <td>2372.0</td>\n",
       "      <td></td>\n",
       "      <td>0.345557</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ADM0_EN  ADM0_EN_ft_style ADM0_PCODE  ADM0_PCODE_ft_style ADM1_EN  \\\n",
       "15283  Ethiopia               0.0         ET                  0.0    Afar   \n",
       "15284  Ethiopia               0.0         ET                  0.0    Afar   \n",
       "15285  Ethiopia               0.0         ET                  0.0    Afar   \n",
       "15286  Ethiopia               0.0         ET                  0.0    Afar   \n",
       "15287  Ethiopia               0.0         ET                  0.0    Afar   \n",
       "\n",
       "      ADM1_PCODE ADM2ALT1EN ADM2ALT2EN                ADM2_EN ADM2_PCODE  ...  \\\n",
       "15283       ET02                           Zone 1 (Awsi Rasu)     ET0201  ...   \n",
       "15284       ET02                         Zone 2 (Kilbet Rasu)     ET0202  ...   \n",
       "15285       ET02                           Zone 3 (Gabi Rasu)     ET0203  ...   \n",
       "15286       ET02                        Zone 4 (Fantana Rasu)     ET0204  ...   \n",
       "15287       ET02                           Zone 5 (Hari Rasu)     ET0205  ...   \n",
       "\n",
       "             date  date_ft_style  \\\n",
       "15283  2019-08-19            0.0   \n",
       "15284  2019-08-19            0.0   \n",
       "15285  2019-08-19            0.0   \n",
       "15286  2019-08-19            0.0   \n",
       "15287  2019-08-19            0.0   \n",
       "\n",
       "                                            geometry_pos  \\\n",
       "15283  {'type': 'Point', 'coordinates': [41.199624, 1...   \n",
       "15284  {'type': 'Point', 'coordinates': [40.5829508, ...   \n",
       "15285  {'type': 'Point', 'coordinates': [40.6370475, ...   \n",
       "15286  {'type': 'Point', 'coordinates': [40.2679372, ...   \n",
       "15287  {'type': 'Point', 'coordinates': [40.3595423, ...   \n",
       "\n",
       "       geometry_vertex_count  import_notes ndvi_mean   qa_mean     validOn  \\\n",
       "15283                 1769.0                0.185094  1.037139  2019-08-27   \n",
       "15284                  986.0                0.192246  1.111855  2019-08-27   \n",
       "15285                 2366.0                0.317286  1.010033  2019-08-27   \n",
       "15286                 1366.0                0.204236  1.001729  2019-08-27   \n",
       "15287                 2372.0                0.345557  1.000000  2019-08-27   \n",
       "\n",
       "       validOn_ft_style validTo  \n",
       "15283               0.0          \n",
       "15284               0.0          \n",
       "15285               0.0          \n",
       "15286               0.0          \n",
       "15287               0.0          \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ndvi_mean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
